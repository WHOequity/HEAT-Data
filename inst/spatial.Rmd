---
title: "Preparing spatial boundaries for the WHO HEAT application"
author: "Zev Ross"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)


library(RSelenium)
library(readr)
library(fs)
library(purrr)
library(sf)
library(dplyr)
library(rmapshaper)
library(stringr)
library(heatdata)
library(rprojroot)
```


This vignette describes the steps required to collect and process the geographic boundary files from DHS for use in the WHO Health Equity Assessment Toolkit application. The basics:

1. Download the geographic files from DHS
2. Create a table with one record per setting-year-source that includes the geographic data
3. Apply some fixes including alterations to the year (some surveys spanned more than one year and the year assigned by DHS does not match that in HEAT data)
4. Simplify the geometries so that they are faster in the application
5. Attempt to join the WHO HEAT subregion names to the DHS subregion names
6. For unmatched subregions, do manual changes to names as needed


# Prepare WHO subregion data for comparison later


**Create a table of subregions with a converted subregion name column**

```{r who_regions}
who_regions <- heatdata::data_heat_raw %>% 
  filter(dimension == "Subnational region") %>% 
  select(setting, iso3, year, source, subgroup) %>% 
  distinct() %>% 
  mutate(
    subgroup_id = as.numeric(str_extract(subgroup, "^[0-9]{2}")),
    subgroup_conv = str_remove(subgroup, "^[0-9]{2} "), #oddly many have leading numbers
    subgroup_conv = standardize_region_text(subgroup_conv)
  )

head(who_regions)
```


**Careful there are some WHO subgroups repeated**


```{r}
count(who_regions, iso3, year, source, subgroup_id) %>% 
  filter(n>1) %>% 
  semi_join(who_regions, .)
```

**Only keep the first one (arranged by subgroup)**

```{r}
who_regions <- who_regions %>% 
  arrange(subgroup) %>% 
  group_by(iso3, year, source, subgroup_id) %>% 
  mutate(row = row_number()) %>% 
  filter(row == 1) %>% 
  select(-row)
```



**Count subregions by setting-year-source**

```{r}
who_region_count <- count(who_regions, setting, iso3, year, source, name = "who_region_count")
```




# Download the shapefiles from DHS

Because this uses a "headless" (automated) browser  it's prone to hiccups that would be time consuming to deal with programatically. So, for now, this piece needs some babysitting. Generally I run it until it fails, identify where it failed and pick it up from there.

## Get the country codes that will be used for downloading

The two-digit country codes are used in the URLs to specify country, this is also used later to include the ISO3 code for linking with the WHO data.

```{r get_codes}
country_dat <- read_csv("https://api.dhsprogram.com/rest/dhs/countries?f=CSV")
codes <- country_dat$DHS_CountryCode
```


## Start a headless browser

This starts a browser that will be used to click and download shapefiles.

```{r, eval = FALSE}
# I was getting a new error with this line, needed to look
# at what slightly older chromedriver I had and use that
# sel <- rsDriver(port = 3000L, browser = "chrome")
# binman::list_versions("chromedriver")
sel <- rsDriver(port = 3000L, chromever = "73.0.3683.68")

```

## Get the shapefile, move and unzip

There is one key function -- `survey_boundary_prep()` that does three things:

1) `survey_boundary_prep(sel, "AL", "~/junk/spatialfiles")`: Takes a country two-digit ID, determines the links available on that country's page and identifies the survey info text on the page (e.g., "Afghanistan DHS 2015") and then, within the function it will:

a) Create a folder with the name of the country abbreviation (e.g., AL) and then create a series of folders with the name of the survey text ("Afghanistan DHS 2015"). Note that the folders are created even before anything is downloaded so a blank folder can determine an issue.

b) Download with `survey_boundary_download(sel, link)` which will download to the default folder (on my machine this is `~/Downloads`).

c) survey_boundary_move_unzip() will take the zip from `~/Downloads` and move it to the appropriate new folders.


**Example: get one shapefile**

```{r, eval = FALSE}
# One boundary, before running, empty the downloads folder
survey_boundary_prep(sel, "BO", "~/junk/spatialfiles")
```

**Example: get many shapefiles (this is what I use)**


Set the location to move the files to

```{r}
spatial_dir <- "/Volumes/GoogleDrive/My Drive/projects/proj-who-heat/big-project-files/dhs-boundary-shapefiles/"
```

Run the downloads


```{r, eval = FALSE}
# this takes some babysitting. I made it an error if no links are found and this causes
# a couple of errors and then otherwise sometimes it just hiccups.
map(codes, function(x) survey_boundary_prep(sel, x, spatial_dir))
```

**Important assumptions about the download**

* The number of and order of the download links and the survey text are the same
* There is only one ZIP file that meets the regular expression rules in the downloads folder. (Though an error is raised if there is already a ZIP)
* ZIP files from DHS all have the same naming pattern.


# Review results of the download process

## Check for empty directories (indicating missing data)

Check the country-level directories and then the survey-level directories to see if any are empty to check on them. I'm doing this by taking all the files, stripping the paths down to the country path and then the survey path. Any country path that only occurs once (or two times?) is empty -- same with surveys.

**Set up table listing country and survey folders**

```{r}
spatial_dir_info <-dir_info(spatial_dir, recursive = TRUE) 
spatial_dir_info$country_folders <- stringr::str_extract(spatial_dir_info$path, "^/Users/zevross/junk/spatialfiles/..")
spatial_dir_info$survey_folders <- stringr::str_extract(spatial_dir_info$path, "^/Users/zevross/junk/spatialfiles/../.* \\d{4} ...")
glimpse(spatial_dir_info)
```

**Which (if any) country folders are empty**

I'm seeing two countries with essentially empty folders: Lao People's Democratic Republic and Papua New Guinea and I double checked and these do not have data online.

```{r}
# I'm seeing two PG and LA and I double-checked these
# Lao People's Democratic Re
count(spatial_dir_info, country_folders) %>% 
  arrange(n) %>% 
  filter(n <= 2)
```
**Which (if any) survey folders are empty**

I'm not seeing any survey folders that don't have files.

```{r}
count(spatial_dir_info, survey_folders) %>% 
  arrange(n) %>% 
  filter(n <= 2)
```



# Begin working with the shapefiles

## Create list of shapefile paths

```{r}
shppaths <- str_subset(spatial_dir_info$path, ".shp$")
head(shppaths)
```

## Convert shapefiles to to a single row in a table

Create tables with one record per country-year-source that includes geography.

**Example: Single shapefile to tibble**

For each survey we want to create a single row in a table with the shapes as a list column.


```{r, eval = FALSE}
shppath <- shppaths[1]
surveyname <- str_extract(shppath, "(?<=shapefiles/[A-Z]{2}/)(.*)(?=/shps)")
DHS_CountryCode <- str_extract(shppath, "(?<=shapefiles/)([A-Z]{2})(?=/)")
shpfilename <- fs::path_file(shppath)
shp <- st_read(shppath,  stringsAsFactors = FALSE) 
res <- shp_to_tibble(shp, DHS_CountryCode,  surveyname, shpfilename)
glimpse(res)
```

**Example: Many shapefiles to tibble (this is what I use)**

The same basic process as the single shapefile process but with a vector of paths. Note that a couple of these shapefiles (Afghanistan 2010 OTH, for example) are very weird and have more than one country. It might only be Afghanistan and Albania that have gotten mixed up. So the `shp_to_tibble()` function will limit to records with the correct DHS country code.

```{r}
shps <- furrr::future_map_dfr(shppaths, function(x){
  surveyname <- stringr::str_extract(x, "(?<=shapefiles/[A-Z]{2}/)(.*)(?=/shps)")
  DHS_CountryCode <- str_extract(x, "(?<=shapefiles/)([A-Z]{2})(?=/)")
  shpfilename <- fs::path_file(x) %>% as.character()
  shp <- st_read(x,  stringsAsFactors = FALSE, quiet= TRUE)
  shp_to_tibble(shp, DHS_CountryCode, surveyname, shpfilename, quiet = TRUE)
  
}, .progress = TRUE)
```

**We want the ISO3 code so link to `country_dat`**

```{r}
shps <- left_join(shps, select(country_dat, DHS_CountryCode, iso3 = ISO3_CountryCode), by = c("DHSCC" = "DHS_CountryCode"))
```

# Limit to just one shapefile per iso3-year-source

Many countries have multiple shapefiles for some reason. This function takes, as input, the subregion count for each iso3-year-source. The selection process for shapefiles is:

1. For some that I know which is correct based on a review I manually drop the wrong ones
2. Otherwise choose the shapefile with the most subregions

Note that I did origially write this piece so that I tried to choose the shapefile with the number of subregions that was closest to the number in WHO HEAT but this had worse results.

```{r}
shps <- choose_one_shp(shps)
```



# Evaluate completeness in comparison to WHO HEAT data

## Set-up for completeness evaluation

**Create table with unique WHO HEAT country-years**

```{r}
library(stringr)
heat_yr_countries <- heatdata::data_heat_raw %>% 
  ungroup() %>% 
  filter(dimension == "Subnational region", source == "DHS") %>% 
  select(setting, iso3, year) %>% 
  distinct() %>% 
  mutate(type = "who")
```

**Create table with unique DHS country-years**

```{r}
dhs_yr_countries <- filter(shps, SVYTYPE == "DHS") %>% 
select(setting = country, year, iso3) %>% 
  distinct() %>% 
  mutate(type = "dhs")
```


## Determine if any countries from WHO HEAT do not exist in DHS data

**WHO countries**

```{r}
who_heat_iso3 <- unique(heat_yr_countries$iso3)
who_heat_iso3
```

**DHS countries**

```{r}
dhs_iso3 <- unique(dhs_yr_countries$iso3)
dhs_iso3
```

**Countries in WHO not in DHS**

If result is `character(0)` then all countries in WHO are in DHS.

```{r}
who_heat_iso3[!who_heat_iso3%in%dhs_iso3]
```
## Determine if any country-years from WHO HEAT do not exist in DHS data

**Join WHO and DHS country-years**

```{r}
yr_country_join <- left_join(heat_yr_countries, dhs_yr_countries, by = c("iso3", "year"))
head(yr_country_join)
```


**Identify country-years that are in WHO but not in DHS**

I see 22 instances. Note that for most of these it's simply a matter of changing the year to make them match, see next section.

```{r}
filter(yr_country_join, is.na(type.y)) %>% 
  select(setting.x, iso3, year) %>% data.frame()
```


# Apply fixes to country names and to survey years

There are two ways to "fix" the country-year mismatch:

1. Change the DHS shapefile year to match WHO: Note that in most of these cases if you go to the [DHS site](https://spatialdata.dhsprogram.com/boundaries/#view=table&countryId=AF) and look at "Survey Details" you'll see that the survey spans more than one year. They picked one to use for the shapefile but the WHO HEAT data is from the other year. So we just manually change the year. For example, Bangladesh-1993 exists in WHO but not in DHS. But there is a DHS 1994 for Bangladesh and under Survey Details the fieldwork was done "November 1993 - March 1994" so we can simply change the year of the DHS shapefile to 1993 and we're done.

2. Cheat, duplicate a previous year's shapefile and hope for the best: This only happens for one country. For Peru, WHO data has many years (1996, 2000, 2004, 2005-2012). But DHS does not have shapefiles for 2005, 2006 or 2008. In this case, for missing years I will use the previous year shapefile assuming they would be correct.

## Change the DHS shapefile year to match WHO


For all but one of the countries the fix is just to change the year, these country-year changes have been programmed into the function.

```{r}
shps <- spatial_fix_survey_years(shps)
```


## Duplicate geographies

For now, this only applies to Peru. But we're basically copying the geography from one year to another.

```{r}
shps <- duplicate_survey_geo(shps)
```



# Simplify the geometries

This creates a new column of `sf_simplified` which is a simplified version of the `sf` column. There is a discussion in Git #4 about the simplification level but for now it's just 0.2 for all.

```{r, eval = FALSE}
# This takes a few minutes
shps <- spatial_simplify_geo(shps, just_testing = TRUE)
```



# Add new country boundaries based on the sub-national boundaries

```{r, eval = FALSE}
# need to review the topological errors
shps <- union_subnational_boundaries(shps, just_testing = TRUE)
```

```{r}
# because takes awhile
# readr::write_rds(shps, "~/junk/shps.rds")
shps <- readr::read_rds("~/junk/shps.rds")
```



**Make sure it works**

```{r, fig.height=4, dev='svg'}
x <- st_geometry(shps$sf_simplified[[1]])
y <- st_geometry(shps$sf_simplified_country[[1]])
plot(x, col = "yellow")
plot(y, add = T, border = "red", lwd = 3)
```


# Drop odd country-years

There are a few situations where the review of the shapefiles and the WHO data showed enough oddities that we should drop.

```{r}
shps <- drop_problematic_country_yr(shps)
```



# Compare subgroup names from WHO to those from DHS

The WHO have provided raw data and the subgroup names do not always match. The goal is to identify non-matches and manually see if small changes to the DHS names can yield proper matches. In some cases, the differences is just a small spelling error.

You'll see below that what we do is:

1. Prepare the DHS and WHO tables with lower case and remove punctuation
2. Try to join them on iso3, year, source (though all sources should be DHS for now)
3. Keep all subgroups from the WHO HEAT data and identify those that do not have a match in the DHS data
4. Export non-matches to an Excel file, `subregion_fixes_in_progress.xlsx`.
5. Review manually by filling in the `REGNAME` and `DHSREGEN` fields if possible and then assigning a "match", "no match" etc. Save reviewed records to the `subregion_fixes_complete.xlsx` file
6. Read in and apply fixes and cycle through. 


**Prepare DHS subgroup data**

There are four columns with a version of the region name. Note that, in the end, it seems like `DHSREGFR` and `DHSREGSP` do not have matching names.

```{r, warning=FALSE}
dhs_regions <- select(shps, setting = country, iso3, year, source = SVYTYPE, sf, survey_info = web_survey_dtl, geo_notes = SVYNOTES) %>% 
  tidyr::unnest() %>% 
  select(setting, iso3, year, source,  survey_info, geo_notes, REGCODE, REGNAME, DHSREGEN, DHSREGFR, DHSREGSP) %>% 
  mutate(non_mappable = str_detect(geo_notes, "non-mappable"),
         REGNAME_conv = standardize_region_text(REGNAME),
         DHSREGEN_conv = standardize_region_text(DHSREGEN),
         DHSREGFR_conv = standardize_region_text(DHSREGFR),
         DHSREGSP_conv = standardize_region_text(DHSREGSP),
         rec = row_number())


dhs_regions$REGNAME_conv[is.na(dhs_regions$REGNAME_conv)] <- "Missing region name"
dhs_regions$DHSREGEN_conv[is.na(dhs_regions$DHSREGEN_conv)] <- "Missing region name"
dhs_regions$DHSREGFR_conv[is.na(dhs_regions$DHSREGFR_conv)] <- "Missing region name"
dhs_regions$DHSREGSP_conv[is.na(dhs_regions$DHSREGSP_conv)] <- "Missing region name"
```




**Limit to DHS for WHO and DHS**

```{r}
who_regions <- filter(who_regions, source == "DHS") %>% 
  mutate(type = "WHO")
dhs_regions <- filter(dhs_regions, source == "DHS") %>% 
  mutate(type = "DHS")
```



**Can we join on ID?**

For the WHO HEAT subregions they start with a number I would have guessed that this could be linked to `REGCODE` in the DHS data. For many this works but for some this completely does not work. So for now the answer to this is "No".


```{r}
# join_vars <- c("iso3", "year", "source")
# x <- left_join(who_regions, dhs_regions, by = c(join_vars, "subgroup_id" = "REGCODE"))
# select(x, subgroup, REGNAME, DHSREGEN)
# select(x, setting.x, year,  subgroup, subgroup_id,  REGNAME, DHSREGEN) %>% 
#   write_csv("~/junk/join_on_ID.csv")
```




**Do initial join of WHO to DHS to determine subregion matches**

I tried the package `fuzzyjoin` but in some cases one digit difference matters and sometimes it doesn't. I also want to join using `REGNAME`, `DHSREGEN`, `DHSREGFR` and `DHSREGSP` though it turns out the that the FR and SP ones do not match any.


```{r}
join_vars <- c("iso3", "year", "source")

joined_regions <- left_join(who_regions, dhs_regions[,c(join_vars, "rec", "type", "REGNAME_conv")], 
                            by = c(join_vars, "subgroup_conv" = "REGNAME_conv"),
                            suffix = c(".who", ".regname")) %>% 
  rename(
    rec.regname = rec,
  )

joined_regions <- left_join(joined_regions, dhs_regions[,c(join_vars, "rec", "type", "DHSREGEN_conv")], 
                            by = c(join_vars, "subgroup_conv" = "DHSREGEN_conv"))%>% 
  rename(
    type.dhsregen = type,
    rec.dhsregen = rec,
  )

joined_regions <- left_join(joined_regions, dhs_regions[,c(join_vars, "rec", "type", "DHSREGFR_conv")], 
                            by = c(join_vars, "subgroup_conv" = "DHSREGFR_conv"))%>% 
  rename(
    type.dhsregfr = type,
    rec.dhsregfr = rec,
  )

joined_regions <- left_join(joined_regions, dhs_regions[,c(join_vars, "rec", "type", "DHSREGSP_conv")], 
                            by = c(join_vars, "subgroup_conv" = "DHSREGSP_conv"))%>% 
  rename(
    type.dhsregsp = type,
    rec.dhsregsp = rec,
  )


joined_regions$join_source <- NA
joined_regions$join_source[!is.na(joined_regions$rec.regname) & is.na(joined_regions$join_source)] <- "REGNAME_conv" 
joined_regions$join_source[!is.na(joined_regions$rec.dhsregen) & is.na(joined_regions$join_source)] <- "DHSREGEN_conv" 
joined_regions$join_source[!is.na(joined_regions$rec.dhsregfr) & is.na(joined_regions$join_source)] <- "DHSREGFR_conv"
joined_regions$join_source[!is.na(joined_regions$rec.dhsregsp) & is.na(joined_regions$join_source)] <- "DHSREGSP_conv"
joined_regions$join_source[is.na(joined_regions$join_source)] <- "No match"

joined_regions$rec_join <- map_int(1:nrow(joined_regions), function(x){
  res <- joined_regions[x,c( "rec.regname", "rec.dhsregen", "rec.dhsregfr", "rec.dhsregsp")] %>% 
    unlist() %>% 
    unique() %>% 
    na.exclude()
  ifelse(length(res)>0, res, NA)
})

joined_regions <- joined_regions %>% ungroup()

joined_regions <- select(joined_regions, setting, iso3, year, source, subgroup, join_source, rec_join)
joined_regions <- left_join(joined_regions, dhs_regions[,c("rec", "REGNAME", "DHSREGEN", "REGCODE")], by = c("rec_join" = "rec"))

glimpse(joined_regions)
```

**Example of matched records**

```{r}
filter(joined_regions, join_source != "No match") %>% glimpse()
```

**Example of non-matched records**

```{r}
filter(joined_regions, join_source == "No match") %>% glimpse()
```

## Create a file for manual fixing


**First create the table of WHO subgroups that do not have a match in DHS data**

```{r}
subregion_fix_needed <- joined_regions %>% 
  filter(join_source == "No match") %>% 
  select(setting, iso3, year, source, subgroup, REGNAME, DHSREGEN) %>% 
  mutate(notes = "") %>% 
  arrange(iso3, year, str_remove(subgroup, "^[0-9]{2} "))
nrow(subregion_fix_needed)
```


**Read our table of fixes**

```{r}
already_fixed_or_reviewed <- readxl::read_excel("../data-raw/subregion_fixes_complete.xlsx") %>% 
  distinct()
nrow(already_fixed_or_reviewed) #153, 194
```


**Two options for handling the "already fixed" records**

1. You can anti join the "fix needed" records with ALL the "already fixed or reviewed" in which case you will be left with only records you have not previously reviewed
2. You can anti join the "fix needed" records with ONLY the "already fixed or reviewed" that match in which case you will be left with records that still do not have a match


**Example: If you only want to drop matched records**

Used

```{r}
matched <- filter(already_fixed_or_reviewed, str_detect(notes, "^match"))
still_matched <- semi_join(matched, dhs_regions, by = c("iso3", "year", "source", "REGNAME", "DHSREGEN"))
subregion_fix_needed <- anti_join(subregion_fix_needed, still_matched, by = c("iso3", "year", "source", "subgroup"))

# if there is an NA or empty field then we have not reviewed it
sum(is.na(subregion_fix_needed$notes) || subregion_fix_needed$notes == "")
```


**Example: If you want to drop previously rev**

Not used

```{r, eval = FALSE}
subregion_fix_needed <- anti_join(subregion_fix_needed, already_fixed_or_reviewed, by = c("iso3", "year", "source", "subgroup"))
nrow(subregion_fix_needed) 
```

**Get the notes field of non-matches to show which you've worked on already**

```{r}
subregion_fix_needed <- left_join(select(subregion_fix_needed, -notes), 
          select(already_fixed_or_reviewed,  -REGNAME, -DHSREGEN), 
          by = c("setting", "iso3", "year", "source", "subgroup"))
```




**Export to a file called `subregion_fixes_inprogress.xlsx`**

Originally we exported to CSV but if a user opens in Excel the encoding would be problematic. So now we export to Excel. The expectation is **that the user will open the "in_progress" version, will make changes and then will copy changes over to the "complete" file.


Export to XLSX

```{r}
writexl::write_xlsx(subregion_fix_needed, "../data-raw/subregion_fixes_in_progress.xlsx")
```

## Manually fix the records

Open the Excel file, then for an ISO3-year, you can use this function to determine which records don't have a match and then the possible matches from the DHS data.


```{r, eval = FALSE}
print_subregions_iso3_year(joined_regions, dhs_regions, "UGA", 2016)
```

**Here are the options for coding the notes field**

* match: high confidence in match
* match uncertain: fairly high confidence but with doubts
* no match: no match
* no match uncertain: no match but it's possible there is a match (not enough certainty in a near match)
* no match part of combined: no match, it appears that the "single" region from WHO is part of a region in DHS that combines multiple smaller regions.


# Final creation of the file


```{r}
final_matches <- readxl::read_excel("../data-raw/subregion_fixes_complete.xlsx") %>% 
  distinct() %>% 
  filter(str_detect(notes, "^match")) %>% 
  semi_join(dhs_regions, by = c("iso3", "year", "source", "REGNAME", "DHSREGEN")) %>% 
  select(-notes)
  

fixes_applied <- inner_join(joined_regions, final_matches, by = c("iso3", "year", "source", "subgroup"), suffix = c("", ".y")) %>% 
  mutate(REGNAME = REGNAME.y,
         DHSREGEN = DHSREGEN.y) %>% 
  select(-REGNAME.y, -DHSREGEN.y, -setting.y, -join_source, -rec_join, -REGCODE)


joined_regions_subtract <- anti_join(joined_regions, fixes_applied, by = c("iso3", "year", "source", "subgroup")) %>% 
  select(-join_source, -rec_join, -REGCODE)
  

final_joined_regions <- bind_rows(
  joined_regions_subtract,
  fixes_applied
)



shps$sf_simplified <- map(1:length(shps$sf_simplified), function(x){
  
  geos <- shps$sf_simplified[[x]]
  n_orig <- nrow(geos)
  iso3_yr_source <- shps[x,]
  for_join <- semi_join(final_joined_regions, iso3_yr_source, by = c("iso3", "year", "source" = "SVYTYPE"))
  
  if(nrow(for_join) == 0){
    geos$subgroup <- NA
  }else{
      
  vals <- left_join(geos, 
            for_join[,c("subgroup", "REGNAME", "DHSREGEN")], by = c("REGNAME", "DHSREGEN"))
  
  }

  if(nrow(geos) != n_orig) stop(glue::glue("stop check {x}, {iso3_yr_source$iso3}, {iso3_yr_source$year}"))
  geos
  
})



```




# National

```{r}
natl <- sf::st_read("../data-raw/raw-spatial-data/national_boundaries/Detailed global map templates (suitable for national and subnational maps)/MapTemplate_detailed_2013/Shapefiles/detailed_2013.shp")
natl$natl_geo_year <- 2013

library(geojsonio)
natl_geojson <- natl %>%
  mutate(
    national_geojson = map(geometry, ~geojson_list(st_sf(st_sfc(st_cast(., "MULTILINESTRING")))))
  ) %>%
  select(iso3 = ISO_3_CODE, natl_geo_year,  national_geojson) %>%
  st_set_geometry(., NULL)

shps <- left_join(shps, natl_geojson, by = "iso3")


```



# Disputed


```{r}
root <- find_root(is_git_root)

disputed_kenya <- sf::read_sf(fs::path(root, "/data-raw/raw-spatial-data/disputed_specific/disputed_kenya_ssudan_git1070.shp"))
disputed_kenya <- geojson_list(disputed_kenya)
disputed_kenya <- tibble(country = c("kenya", "south sudan"),
       geojson = list(disputed_kenya, disputed_kenya))

disputed_all <- disputed_kenya

disputed_egypt <- sf::read_sf(fs::path(root, "data-raw/raw-spatial-data/disputed_specific/disputed_egypt_sudan_git1069_3rd.shp"))
disputed_egypt <- geojson_list(disputed_egypt)


disputed_egypt <- tibble(country = c("egypt", "sudan"),
                       geojson = list(disputed_egypt, disputed_egypt))

disputed_all <- rbind(disputed_all, disputed_egypt)

country_dat$lower_country <- tolower(country_dat$CountryName)

disputed_all <- left_join(disputed_all, select(country_dat, lower_country, iso3 = ISO3_CountryCode), by = c("country" = "lower_country"))
disputed_all <- rename(disputed_all, disputed_geojson = geojson) %>% 
  select(-country)

shps <- left_join(shps, disputed_all, by = "iso3")
```


# Renaming


```{r}
shps <- shps %>% 
  rename(
    subnational_detailed_geojson = sf,
    subnational_simplified_geojson = sf_simplified,
    national_usingsubnatl_geojson = sf_simplified_country,
    national_original_geojson = national_geojson
  ) 



matches2 <- function (match, ignore.case = TRUE, vars = current_vars())
{
  tidyselect:::grep_vars(match, vars, ignore.case = ignore.case, perl = TRUE)
}

shps <- select(shps, matches2("^((?!geojson).)*$"), tidyselect::matches("_geojson"))

write_rds(shps, path = "../data-raw/spatial_data_pre_convert.rds")


shps <- select(shps, -CNTRYNAMEF, -SVYNOTES, -subnational_detailed_geojson, -web_survey_dtl, -shapefile_name, -n_geometries, -n_geometries_removed, -shapefile_count) %>% 
  mutate(
    subnational_simplified_geojson = map(subnational_simplified_geojson, geojson_list)
  )

x <- select(shps, 1:4, subnational_simplified_geojson)

write_rds(x, "~/junk/junk_shps.rds", compress = "gz")
```

# filter out manually

```{r}
spatial_subnational_simplified <- spatial_subnational_simplified %>% 
  filter(!setting %in% c("India", "Pakistan", "Comoros", "Gambia", "Haiti", "Maldives", "Sao Tome and Principe", "Yemen"))

spatial_national <- spatial_national %>% 
  filter(!setting %in% c("India", "Pakistan", "Comoros", "Gambia", "Haiti", "Maldives", "Sao Tome and Principe", "Yemen"))

```



# All in one place

```{r, message=FALSE, eval = FALSE}

library(readr)
library(fs)
library(purrr)
library(sf)
library(dplyr)
library(rmapshaper)
library(stringr)
library(heatdata)
library(geojsonio)


#************************************************
# Country info ----
#************************************************

country_dat <- read_csv("https://api.dhsprogram.com/rest/dhs/countries?f=CSV")
codes <- country_dat$DHS_CountryCode


#************************************************
# Download geographic files ----
#************************************************

# See code above, we don't need to do this often


#************************************************
# Unique WHO subregions ----
#************************************************

who_regions <- heatdata::data_heat_raw %>% 
  filter(dimension == "Subnational region") %>% 
  select(setting, iso3, year, source, subgroup) %>% 
  distinct() %>% 
  mutate(
    subgroup_id = as.numeric(str_extract(subgroup, "^[0-9]{2}")),
    subgroup_conv = str_remove(subgroup, "^[0-9]{2} "), #oddly many have leading numbers
    subgroup_conv = standardize_region_text(subgroup_conv)
  )

who_regions <- who_regions %>% 
  arrange(subgroup) %>% 
  group_by(iso3, year, source, subgroup_id) %>% 
  mutate(row = row_number()) %>% 
  filter(row == 1) %>% 
  select(-row)


#************************************************
# Read in and format shapefiles ----
#************************************************



spatial_dir <- "/Volumes/GoogleDrive/My Drive/projects/proj-who-heat/big-project-files/dhs-boundary-shapefiles/"
spatial_dir_info <-dir_info(spatial_dir, recursive = TRUE) 
spatial_dir_info$country_folders <- stringr::str_extract(spatial_dir_info$path, "^/Users/zevross/junk/spatialfiles/..")
spatial_dir_info$survey_folders <- stringr::str_extract(spatial_dir_info$path, "^/Users/zevross/junk/spatialfiles/../.* \\d{4} ...")
shppaths <- str_subset(spatial_dir_info$path, ".shp$")

shps <- furrr::future_map_dfr(shppaths, function(x){
  surveyname <- stringr::str_extract(x, "(?<=shapefiles/[A-Z]{2}/)(.*)(?=/shps)")
  DHS_CountryCode <- str_extract(x, "(?<=shapefiles/)([A-Z]{2})(?=/)")
  shpfilename <- fs::path_file(x) %>% as.character()
  shp <- st_read(x,  stringsAsFactors = FALSE, quiet= TRUE)
  shp_to_tibble(shp, DHS_CountryCode, surveyname, shpfilename, quiet = TRUE)
  
}, .progress = TRUE)

write_rds()
#************************************************
# Fix the geographic files ----
#************************************************

# 1. Limit to one shape per country
# 2. Fix some spatial survey years
# 3. Remove duplicate survey info
# 4. Simplify the geography
# 5. Union the subnational boundaries
# 6. Drop geographies we know are problems


shps <- left_join(shps, select(country_dat, DHS_CountryCode, iso3 = ISO3_CountryCode), by = c("DHSCC" = "DHS_CountryCode"))
shps <- choose_one_shp(shps)
shps <- spatial_fix_survey_years(shps)
shps <- duplicate_survey_geo(shps)
shps <- spatial_simplify_geo(shps,simplification_level = 0.75, just_testing = TRUE)
shps <- union_subnational_boundaries(shps, just_testing = TRUE)
shps <- drop_problematic_country_yr(shps)

readr::write_rds(shps, "~/junk/shps.rds")


#************************************************
# Prepare to link geo subregions with who subregions ----
#************************************************


dhs_regions <- select(shps, setting = country, iso3, year, source = SVYTYPE, sf, survey_info = web_survey_dtl, geo_notes = SVYNOTES) %>% 
  tidyr::unnest() %>% 
  select(setting, iso3, year, source,  survey_info, geo_notes, REGCODE, REGNAME, DHSREGEN, DHSREGFR, DHSREGSP) %>% 
  mutate(non_mappable = str_detect(geo_notes, "non-mappable"),
         REGNAME_conv = standardize_region_text(REGNAME),
         DHSREGEN_conv = standardize_region_text(DHSREGEN),
         DHSREGFR_conv = standardize_region_text(DHSREGFR),
         DHSREGSP_conv = standardize_region_text(DHSREGSP),
         rec = row_number())


dhs_regions$REGNAME_conv[is.na(dhs_regions$REGNAME_conv)] <- "Missing region name"
dhs_regions$DHSREGEN_conv[is.na(dhs_regions$DHSREGEN_conv)] <- "Missing region name"
dhs_regions$DHSREGFR_conv[is.na(dhs_regions$DHSREGFR_conv)] <- "Missing region name"
dhs_regions$DHSREGSP_conv[is.na(dhs_regions$DHSREGSP_conv)] <- "Missing region name"

who_regions <- filter(who_regions, source == "DHS") %>% 
  mutate(type = "WHO")
dhs_regions <- filter(dhs_regions, source == "DHS") %>% 
  mutate(type = "DHS")


#************************************************
# Join WHO with DHS-geo and find mismatches ----
#************************************************


join_vars <- c("iso3", "year", "source")

joined_regions <- left_join(who_regions, dhs_regions[,c(join_vars, "rec", "type", "REGNAME_conv")], 
                            by = c(join_vars, "subgroup_conv" = "REGNAME_conv"),
                            suffix = c(".who", ".regname")) %>% 
  rename(
    rec.regname = rec,
  )

joined_regions <- left_join(joined_regions, dhs_regions[,c(join_vars, "rec", "type", "DHSREGEN_conv")], 
                            by = c(join_vars, "subgroup_conv" = "DHSREGEN_conv"))%>% 
  rename(
    type.dhsregen = type,
    rec.dhsregen = rec,
  )

joined_regions <- left_join(joined_regions, dhs_regions[,c(join_vars, "rec", "type", "DHSREGFR_conv")], 
                            by = c(join_vars, "subgroup_conv" = "DHSREGFR_conv"))%>% 
  rename(
    type.dhsregfr = type,
    rec.dhsregfr = rec,
  )

joined_regions <- left_join(joined_regions, dhs_regions[,c(join_vars, "rec", "type", "DHSREGSP_conv")], 
                            by = c(join_vars, "subgroup_conv" = "DHSREGSP_conv"))%>% 
  rename(
    type.dhsregsp = type,
    rec.dhsregsp = rec,
  )


joined_regions$join_source <- NA
joined_regions$join_source[!is.na(joined_regions$rec.regname) & is.na(joined_regions$join_source)] <- "REGNAME_conv" 
joined_regions$join_source[!is.na(joined_regions$rec.dhsregen) & is.na(joined_regions$join_source)] <- "DHSREGEN_conv" 
joined_regions$join_source[!is.na(joined_regions$rec.dhsregfr) & is.na(joined_regions$join_source)] <- "DHSREGFR_conv"
joined_regions$join_source[!is.na(joined_regions$rec.dhsregsp) & is.na(joined_regions$join_source)] <- "DHSREGSP_conv"
joined_regions$join_source[is.na(joined_regions$join_source)] <- "No match"

joined_regions$rec_join <- map_int(1:nrow(joined_regions), function(x){
  res <- joined_regions[x,c( "rec.regname", "rec.dhsregen", "rec.dhsregfr", "rec.dhsregsp")] %>% 
    unlist() %>% 
    unique() %>% 
    na.exclude()
  ifelse(length(res)>0, res, NA)
})

joined_regions <- joined_regions %>% ungroup()

joined_regions <- select(joined_regions, setting, iso3, year, source, subgroup, join_source, rec_join)
joined_regions <- left_join(joined_regions, dhs_regions[,c("rec", "REGNAME", "DHSREGEN", "REGCODE")], by = c("rec_join" = "rec"))


#************************************************
# Find mismatches ----
#************************************************


subregion_fix_needed <- joined_regions %>% 
  filter(join_source == "No match") %>% 
  select(setting, iso3, year, source, subgroup, REGNAME, DHSREGEN) %>% 
  mutate(notes = "") %>% 
  arrange(iso3, year, str_remove(subgroup, "^[0-9]{2} "))


# ----- bring in a file with fixes we've already identified to remove from fix_needed

already_fixed_or_reviewed <- readxl::read_excel("../data-raw/subregion_fixes_complete.xlsx") %>% 
  distinct()

matched <- filter(already_fixed_or_reviewed, str_detect(notes, "^match"))
still_matched <- semi_join(matched, dhs_regions, by = c("iso3", "year", "source", "REGNAME", "DHSREGEN"))
subregion_fix_needed <- anti_join(subregion_fix_needed, still_matched, by = c("iso3", "year", "source", "subgroup"))

subregion_fix_needed <- left_join(select(subregion_fix_needed, -notes), 
                                  select(already_fixed_or_reviewed, -REGNAME, -DHSREGEN), 
                                  by = c("setting", "iso3", "year", "source", "subgroup")) %>% 
  arrange(iso3)

# ----- Create a file where you actively make fixes

# You might want to run this line of code
#subregion_fix_needed <- filter(subregion_fix_needed, is.na(notes))
writexl::write_xlsx(subregion_fix_needed, "../data-raw/subregion_fixes_in_progress.xlsx")


# ----- This function helps with fixes. You should have subregion_fixes_in_progress.xlsx
# ----- open and then run this for places with mismatches to help identify possible matches

#print_subregions_iso3_year(joined_regions, dhs_regions, "IND", 2015)



#************************************************
# Update the data to include the fixes ----
#************************************************


final_matches <- readxl::read_excel("../data-raw/subregion_fixes_complete.xlsx") %>% 
  distinct() %>% 
  filter(str_detect(notes, "^match")) %>% 
  semi_join(dhs_regions, by = c("iso3", "year", "source", "REGNAME", "DHSREGEN")) %>% 
  select(-notes)
  

fixes_applied <- inner_join(joined_regions, final_matches, by = c("iso3", "year", "source", "subgroup"), suffix = c("", ".y")) %>% 
  mutate(REGNAME = REGNAME.y,
         DHSREGEN = DHSREGEN.y) %>% 
  select(-REGNAME.y, -DHSREGEN.y, -setting.y, -join_source, -rec_join, -REGCODE)


joined_regions_subtract <- anti_join(joined_regions, fixes_applied, by = c("iso3", "year", "source", "subgroup")) %>% 
  select(-join_source, -rec_join, -REGCODE)
  

final_joined_regions <- bind_rows(
  joined_regions_subtract,
  fixes_applied
)


#************************************************
# Update the simplified subregions with the new ----
# names where applicable
#************************************************

shps$sf_simplified <- map(1:length(shps$sf_simplified), function(x){
  
  geos <- shps$sf_simplified[[x]]
  n_orig <- nrow(geos)
  iso3_yr_source <- shps[x,]
  for_join <- semi_join(final_joined_regions, iso3_yr_source, by = c("iso3", "year", "source" = "SVYTYPE"))
  
  if(nrow(for_join) == 0){
    geos$subgroup <- NA
  }else{
      
  vals <- left_join(geos, 
            for_join[,c("subgroup", "REGNAME", "DHSREGEN")], by = c("REGNAME", "DHSREGEN"))
  
  }

  if(nrow(geos) != n_orig) stop(glue::glue("stop check {x}, {iso3_yr_source$iso3}, {iso3_yr_source$year}"))
  geos
  
})


#************************************************
# Bring in National data ----
#************************************************

natl <- sf::st_read("../data-raw/raw-spatial-data/national_boundaries/Detailed global map templates (suitable for national and subnational maps)/MapTemplate_detailed_2013/Shapefiles/detailed_2013.shp")
natl$natl_geo_year <- 2013

natl <- natl %>%
  mutate(
    sf_national = map(geometry, ~st_sf(st_sfc(st_cast(., "MULTILINESTRING"))))
  ) %>%
  select(iso3 = ISO_3_CODE, natl_geo_year,  sf_national) %>% 
  st_set_geometry(., NULL)


shps <- shps %>% 
  left_join(natl, by = "iso3")



#************************************************
# Bring in disputed regions ----
#************************************************

root <- find_root(is_git_root)

disputed_kenya <- sf::read_sf(fs::path(root, "/data-raw/raw-spatial-data/disputed_specific/disputed_kenya_ssudan_git1070.shp"))
disputed_kenya <- tibble(country = c("kenya", "south sudan"),
       sf_disputed = list(disputed_kenya, disputed_kenya))

disputed_all <- disputed_kenya

disputed_egypt <- sf::read_sf(fs::path(root, "data-raw/raw-spatial-data/disputed_specific/disputed_egypt_sudan_git1069_3rd.shp"))

disputed_egypt <- tibble(country = c("egypt", "sudan"),
                       sf_disputed = list(disputed_egypt, disputed_egypt))

disputed_all <- rbind(disputed_all, disputed_egypt)

country_dat$lower_country <- tolower(country_dat$CountryName)

disputed_all <- left_join(disputed_all, select(country_dat, lower_country, iso3 = ISO3_CountryCode), by = c("country" = "lower_country"))

disputed_all$iso3[disputed_all$country == "south sudan"] <- "SSD"

disputed_all <- select(disputed_all, -country)

shps <- left_join(shps, disputed_all, by = "iso3")

shps_save <- shps

#************************************************
# Do a little cleaning ----
#************************************************

shps <- shps %>% 
  rename(
    dhs_country_code = DHSCC,
    source = SVYTYPE,
    setting = country,
    dhs_survey_notes = SVYNOTES,
    dhs_survey_info = web_survey_dtl,
    n_subregions = n_geometries,
    n_subregions_removed = n_geometries_removed,
    sf_subnational_detailed = sf,
    sf_subnational_simplified = sf_simplified,
    sf_national_usingsubnatl = sf_simplified_country
  ) %>% 
  select(setting, iso3, dhs_country_code, year, source,
         dhs_survey_notes,
         dhs_survey_info,
         shapefile_name,
         n_subregions,
         n_subregions_removed,
         shapefile_count,
         original_survey_year,
         natl_geo_year,
         sf_subnational_detailed,
         sf_subnational_simplified,
         sf_national_usingsubnatl,
         sf_national,
         sf_disputed
         )


# dplyr::filter(shps, setting == "Afghanistan",  year == "2015") %>% 
#   pull(sf_subnational_detailed) %>% 
#   .[[1]] %>% 
#   st_geometry() %>% 
#   plot()

  


shps <- shps %>% 
  mutate(geojson_subnational_detailed = map(sf_subnational_detailed, geojson_list),
         geojson_subnational_simplified = map(sf_subnational_simplified, geojson_list),
         geojson_national_usingsubnatl = map(sf_national_usingsubnatl, geojson_list),
         geojson_disputed = map_if(sf_disputed, function(x){!is.null(x)}, geojson_list),
         geojson_national = map(sf_national, geojson_list)
         )

readr::write_rds(shps, "/Volumes/GoogleDrive/My Drive/projects/proj-who-heat/big-project-files/dhs-processed-geography/dhs-processed-geography.rds", 
                 compress = "gz")

subnatl_dtl <- select(shps, setting, iso3, year, geojson = geojson_subnational_detailed)
subnatl <- select(shps, setting, iso3, year, geojson = geojson_subnational_simplified)
natl <- select(shps, setting, iso3, geojson = geojson_national)
natl_union <- select(shps, setting, iso3, geojson = geojson_national_usingsubnatl)
disputed <- select(shps, setting, iso3, geojson = geojson_disputed)


```


